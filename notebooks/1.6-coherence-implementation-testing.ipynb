{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 60;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 61;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport torch\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence_v2 import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport torch\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence_v2 import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import torch\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from utils.metrics import windowdiff, pk\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence_v2 import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 62;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 63;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 64;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e84fe0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 65;\n",
       "                var nbb_unformatted_code = \"all_samples = [item for sublist in all_segments for item in sublist]\";\n",
       "                var nbb_formatted_code = \"all_samples = [item for sublist in all_segments for item in sublist]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_samples = [item for sublist in all_segments for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e6918d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13678, 92833, 6.78703026758298)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 66;\n",
       "                var nbb_unformatted_code = \"len(all_segments), len(all_samples), (len(all_samples) / len(all_segments))\";\n",
       "                var nbb_formatted_code = \"len(all_segments), len(all_samples), (len(all_samples) / len(all_segments))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(all_segments), len(all_samples), (len(all_samples) / len(all_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e420ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 69;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\nmax_words_per_step = 3\\ncoherence = Coherence(max_words_per_step=max_words_per_step, kb_embeddings=True)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\nmax_words_per_step = 3\\ncoherence = Coherence(max_words_per_step=max_words_per_step, kb_embeddings=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "max_words_per_step = 3\n",
    "coherence = Coherence(max_words_per_step=max_words_per_step, kb_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 57;\n",
       "                var nbb_unformatted_code = \"def get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map,\\n    keywords_current,\\n    suppress_errors=False,\\n    same_word_multiplier=2,  # if set to 1, don't amplify the same words found\\n    no_same_word_penalty=2,  # if set to 1, don't penalize for not finding the same word.\\n):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n                second_word_importance = second_word_tuple[1]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    if same_word_multiplier > 1:\\n                        flattened_coherence_words_only = [\\n                            element[0]\\n                            for sublist in coherence_map\\n                            for element in sublist\\n                        ]\\n\\n                        num_occurrences = flattened_coherence_words_only.count(\\n                            second_word\\n                        )\\n\\n                        if num_occurrences > 0:\\n                            # amplify words that are found as duplicates in the coherence map\\n                            # if the word shows up 1 time, amplify the weight by 2 times\\n                            weighting_multiplier = flattened_coherence_words_only.count(\\n                                second_word\\n                            ) + (same_word_multiplier - 1)\\n                        else:\\n                            # no same word penalty\\n                            weighting_multiplier = (\\n                                1 / no_same_word_penalty\\n                            )  # reduce the importance of this word\\n\\n                    else:\\n                        weighting_multiplier = 1  # set to 1 in case this is turned off.\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = (weighting_multiplier * 2) / (i + 1)\\n                    else:\\n                        weight = (weighting_multiplier * 1) / (i + 1)\\n\\n                    # multiply the weighting factor by the importance of the second word\\n                    weight *= second_word_importance\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * coherence.embedding_lib.get_similarity(\\n                                torch.Tensor(word_one_emb), torch.Tensor(word_two_emb)\\n                            ),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=128,\\n    max_str_length=30,\\n    prediction_thresh=0.48,\\n    coherence_threshold=0.2,\\n    pruning=1,  # remove one sentence worth of keywords\\n    pruning_min=6,  # remove the first sentence in the coherence map once it grows passed 6\\n    dynamic_threshold=False,\\n    coherence_dump_on_prediction=False,\\n    threshold_warmup=10,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n    batch_size=10,\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n\\n    prev_sentence = None\\n\\n    # set up batching\\n    for batch_num in range(0, len(text_data) // batch_size):\\n        # create the current batch to iterate over.\\n        # this method relies on previous sentence as it always keeps track\\n        curr_batch = text_data[\\n            batch_num * batch_size : batch_num * batch_size + batch_size\\n        ]\\n\\n        curr_batch_labels = text_labels[\\n            batch_num * batch_size : batch_num * batch_size + batch_size\\n        ]\\n\\n        for i, (row, label) in enumerate(zip(curr_batch, curr_batch_labels)):\\n            threshold = prediction_thresh\\n            if dynamic_threshold and (i + 1) > threshold_warmup:\\n                last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n                last_n_thresholds.sort()\\n                mid = len(last_n_thresholds) // 2\\n                threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n                print(f\\\"median threshold: {threshold}\\\")\\n            # compare the current sentence to the previous one\\n            if prev_sentence is None:\\n                predictions.append(\\n                    (torch.tensor(0, dtype=torch.int8), 0)\\n                )  # predict a 0 since it's the start\\n                print(f\\\"Label: {label}, Prediction: {0}\\\")\\n                prev_sentence = row\\n                pass\\n            else:\\n                print(f\\\"Sample Number: {i}\\\")\\n\\n                row = truncate_by_token(row, max_tokens)\\n                prev_row = truncate_by_token(prev_sentence, max_tokens)\\n\\n                cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                    [row, prev_row], coherence_threshold=0\\n                )\\n\\n                # add the keywords to the coherence map\\n                coherence_map.append(cohesion)\\n                if pruning > 0 and len(coherence_map) >= pruning_min:\\n                    print(\\\"pruning...\\\", len(coherence_map))\\n                    coherence_map = coherence_map[\\n                        pruning:\\n                    ]  # remove the pruning amount from the beginning of the list\\n                    print(\\\"done pruning...\\\", len(coherence_map))\\n\\n                # truncate the strings for printing\\n                truncated_row = truncate_string(row, max_str_length)\\n                truncated_prev_row = truncate_string(prev_row, max_str_length)\\n                print(\\n                    f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n                )\\n\\n                # compute the word comparisons between the previous (with the coherence map)\\n                # and the current (possibly the first sentence in a new segment)\\n                word_comparisons_with_coherence, weights = compare_coherent_words(\\n                    [*coherence_map, keywords_prev], keywords_current\\n                )\\n\\n                similarities_with_coherence = [\\n                    comparison[2] for comparison in word_comparisons_with_coherence\\n                ]\\n                avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                    len(similarities_with_coherence) or 1\\n                )\\n                weighted_avg_similarity_with_coherence = get_weighted_average(\\n                    similarities_with_coherence, weights\\n                )\\n                print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n                # if the two sentences are similar, create a cohesive prediction\\n                # otherwise, predict a new segment\\n                if weighted_avg_similarity_with_coherence > threshold:\\n                    print(\\n                        f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                    )\\n                    predictions.append((weighted_avg_similarity_with_coherence, 0))\\n                else:\\n                    if coherence_dump_on_prediction:\\n                        # start of a new segment, empty the map\\n                        coherence_map = []\\n                    print(\\n                        f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                    )\\n                    predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n                thresholds.append(weighted_avg_similarity_with_coherence)\\n                print(\\\"===============================================\\\")\\n\\n                prev_sentence = row\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"def get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(\\n    coherence_map,\\n    keywords_current,\\n    suppress_errors=False,\\n    same_word_multiplier=2,  # if set to 1, don't amplify the same words found\\n    no_same_word_penalty=2,  # if set to 1, don't penalize for not finding the same word.\\n):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n                second_word_importance = second_word_tuple[1]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    if same_word_multiplier > 1:\\n                        flattened_coherence_words_only = [\\n                            element[0]\\n                            for sublist in coherence_map\\n                            for element in sublist\\n                        ]\\n\\n                        num_occurrences = flattened_coherence_words_only.count(\\n                            second_word\\n                        )\\n\\n                        if num_occurrences > 0:\\n                            # amplify words that are found as duplicates in the coherence map\\n                            # if the word shows up 1 time, amplify the weight by 2 times\\n                            weighting_multiplier = flattened_coherence_words_only.count(\\n                                second_word\\n                            ) + (same_word_multiplier - 1)\\n                        else:\\n                            # no same word penalty\\n                            weighting_multiplier = (\\n                                1 / no_same_word_penalty\\n                            )  # reduce the importance of this word\\n\\n                    else:\\n                        weighting_multiplier = 1  # set to 1 in case this is turned off.\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = (weighting_multiplier * 2) / (i + 1)\\n                    else:\\n                        weight = (weighting_multiplier * 1) / (i + 1)\\n\\n                    # multiply the weighting factor by the importance of the second word\\n                    weight *= second_word_importance\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * coherence.embedding_lib.get_similarity(\\n                                torch.Tensor(word_one_emb), torch.Tensor(word_two_emb)\\n                            ),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data,\\n    text_labels,\\n    max_tokens=128,\\n    max_str_length=30,\\n    prediction_thresh=0.48,\\n    coherence_threshold=0.2,\\n    pruning=1,  # remove one sentence worth of keywords\\n    pruning_min=6,  # remove the first sentence in the coherence map once it grows passed 6\\n    dynamic_threshold=False,\\n    coherence_dump_on_prediction=False,\\n    threshold_warmup=10,  # number of iterations before using dynamic threshold\\n    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\\n    batch_size=10,\\n):\\n    coherence_map = []\\n    predictions = []\\n    thresholds = []\\n\\n    prev_sentence = None\\n\\n    # set up batching\\n    for batch_num in range(0, len(text_data) // batch_size):\\n        # create the current batch to iterate over.\\n        # this method relies on previous sentence as it always keeps track\\n        curr_batch = text_data[\\n            batch_num * batch_size : batch_num * batch_size + batch_size\\n        ]\\n\\n        curr_batch_labels = text_labels[\\n            batch_num * batch_size : batch_num * batch_size + batch_size\\n        ]\\n\\n        for i, (row, label) in enumerate(zip(curr_batch, curr_batch_labels)):\\n            threshold = prediction_thresh\\n            if dynamic_threshold and (i + 1) > threshold_warmup:\\n                last_n_thresholds = thresholds[(0 - last_n_threshold) :]\\n                last_n_thresholds.sort()\\n                mid = len(last_n_thresholds) // 2\\n                threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\\n                print(f\\\"median threshold: {threshold}\\\")\\n            # compare the current sentence to the previous one\\n            if prev_sentence is None:\\n                predictions.append(\\n                    (torch.tensor(0, dtype=torch.int8), 0)\\n                )  # predict a 0 since it's the start\\n                print(f\\\"Label: {label}, Prediction: {0}\\\")\\n                prev_sentence = row\\n                pass\\n            else:\\n                print(f\\\"Sample Number: {i}\\\")\\n\\n                row = truncate_by_token(row, max_tokens)\\n                prev_row = truncate_by_token(prev_sentence, max_tokens)\\n\\n                cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                    [row, prev_row], coherence_threshold=0\\n                )\\n\\n                # add the keywords to the coherence map\\n                coherence_map.append(cohesion)\\n                if pruning > 0 and len(coherence_map) >= pruning_min:\\n                    print(\\\"pruning...\\\", len(coherence_map))\\n                    coherence_map = coherence_map[\\n                        pruning:\\n                    ]  # remove the pruning amount from the beginning of the list\\n                    print(\\\"done pruning...\\\", len(coherence_map))\\n\\n                # truncate the strings for printing\\n                truncated_row = truncate_string(row, max_str_length)\\n                truncated_prev_row = truncate_string(prev_row, max_str_length)\\n                print(\\n                    f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n                )\\n\\n                # compute the word comparisons between the previous (with the coherence map)\\n                # and the current (possibly the first sentence in a new segment)\\n                word_comparisons_with_coherence, weights = compare_coherent_words(\\n                    [*coherence_map, keywords_prev], keywords_current\\n                )\\n\\n                similarities_with_coherence = [\\n                    comparison[2] for comparison in word_comparisons_with_coherence\\n                ]\\n                avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                    len(similarities_with_coherence) or 1\\n                )\\n                weighted_avg_similarity_with_coherence = get_weighted_average(\\n                    similarities_with_coherence, weights\\n                )\\n                print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n                # if the two sentences are similar, create a cohesive prediction\\n                # otherwise, predict a new segment\\n                if weighted_avg_similarity_with_coherence > threshold:\\n                    print(\\n                        f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                    )\\n                    predictions.append((weighted_avg_similarity_with_coherence, 0))\\n                else:\\n                    if coherence_dump_on_prediction:\\n                        # start of a new segment, empty the map\\n                        coherence_map = []\\n                    print(\\n                        f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                    )\\n                    predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n                thresholds.append(weighted_avg_similarity_with_coherence)\\n                print(\\\"===============================================\\\")\\n\\n                prev_sentence = row\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_weighted_average(weighted_similarities, weights):\n",
    "    return sum(weighted_similarities) / sum(weights)\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(\n",
    "    coherence_map,\n",
    "    keywords_current,\n",
    "    suppress_errors=False,\n",
    "    same_word_multiplier=2,  # if set to 1, don't amplify the same words found\n",
    "    no_same_word_penalty=2,  # if set to 1, don't penalize for not finding the same word.\n",
    "):\n",
    "    word_comparisons = []\n",
    "    weights = []\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "                second_word_importance = second_word_tuple[1]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "\n",
    "                    if same_word_multiplier > 1:\n",
    "                        flattened_coherence_words_only = [\n",
    "                            element[0]\n",
    "                            for sublist in coherence_map\n",
    "                            for element in sublist\n",
    "                        ]\n",
    "\n",
    "                        num_occurrences = flattened_coherence_words_only.count(\n",
    "                            second_word\n",
    "                        )\n",
    "\n",
    "                        if num_occurrences > 0:\n",
    "                            # amplify words that are found as duplicates in the coherence map\n",
    "                            # if the word shows up 1 time, amplify the weight by 2 times\n",
    "                            weighting_multiplier = flattened_coherence_words_only.count(\n",
    "                                second_word\n",
    "                            ) + (same_word_multiplier - 1)\n",
    "                        else:\n",
    "                            # no same word penalty\n",
    "                            weighting_multiplier = (\n",
    "                                1 / no_same_word_penalty\n",
    "                            )  # reduce the importance of this word\n",
    "\n",
    "                    else:\n",
    "                        weighting_multiplier = 1  # set to 1 in case this is turned off.\n",
    "\n",
    "                    # this weight is a recipricol function that will grow smaller the further the keywords are away\n",
    "                    # we want to put more importance on the current words, so we apply twice as much weight.\n",
    "                    if i == 0:\n",
    "                        weight = (weighting_multiplier * 2) / (i + 1)\n",
    "                    else:\n",
    "                        weight = (weighting_multiplier * 1) / (i + 1)\n",
    "\n",
    "                    # multiply the weighting factor by the importance of the second word\n",
    "                    weight *= second_word_importance\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            weight\n",
    "                            * coherence.embedding_lib.get_similarity(\n",
    "                                torch.Tensor(word_one_emb), torch.Tensor(word_two_emb)\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                    weights.append(weight)\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons, weights\n",
    "\n",
    "\n",
    "# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\n",
    "def coherence_tester(\n",
    "    text_data,\n",
    "    text_labels,\n",
    "    max_tokens=128,\n",
    "    max_str_length=30,\n",
    "    prediction_thresh=0.48,\n",
    "    coherence_threshold=0.2,\n",
    "    pruning=1,  # remove one sentence worth of keywords\n",
    "    pruning_min=6,  # remove the first sentence in the coherence map once it grows passed 6\n",
    "    dynamic_threshold=False,\n",
    "    coherence_dump_on_prediction=False,\n",
    "    threshold_warmup=10,  # number of iterations before using dynamic threshold\n",
    "    last_n_threshold=5,  # will only consider the last n thresholds for dynamic threshold\n",
    "    batch_size=10,\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    thresholds = []\n",
    "\n",
    "    prev_sentence = None\n",
    "\n",
    "    # set up batching\n",
    "    for batch_num in range(0, len(text_data) // batch_size):\n",
    "        # create the current batch to iterate over.\n",
    "        # this method relies on previous sentence as it always keeps track\n",
    "        curr_batch = text_data[\n",
    "            batch_num * batch_size : batch_num * batch_size + batch_size\n",
    "        ]\n",
    "\n",
    "        curr_batch_labels = text_labels[\n",
    "            batch_num * batch_size : batch_num * batch_size + batch_size\n",
    "        ]\n",
    "\n",
    "        for i, (row, label) in enumerate(zip(curr_batch, curr_batch_labels)):\n",
    "            threshold = prediction_thresh\n",
    "            if dynamic_threshold and (i + 1) > threshold_warmup:\n",
    "                last_n_thresholds = thresholds[(0 - last_n_threshold) :]\n",
    "                last_n_thresholds.sort()\n",
    "                mid = len(last_n_thresholds) // 2\n",
    "                threshold = (last_n_thresholds[mid] + last_n_thresholds[~mid]) / 2\n",
    "                print(f\"median threshold: {threshold}\")\n",
    "            # compare the current sentence to the previous one\n",
    "            if prev_sentence is None:\n",
    "                predictions.append(\n",
    "                    (torch.tensor(0, dtype=torch.int8), 0)\n",
    "                )  # predict a 0 since it's the start\n",
    "                print(f\"Label: {label}, Prediction: {0}\")\n",
    "                prev_sentence = row\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"Sample Number: {i}\")\n",
    "\n",
    "                row = truncate_by_token(row, max_tokens)\n",
    "                prev_row = truncate_by_token(prev_sentence, max_tokens)\n",
    "\n",
    "                cohesion, keywords_prev, keywords_current = coherence.get_coherence(\n",
    "                    [row, prev_row], coherence_threshold=coherence_threshold\n",
    "                )\n",
    "\n",
    "                # add the keywords to the coherence map\n",
    "                coherence_map.append(cohesion)\n",
    "                if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                    print(\"pruning...\", len(coherence_map))\n",
    "                    coherence_map = coherence_map[\n",
    "                        pruning:\n",
    "                    ]  # remove the pruning amount from the beginning of the list\n",
    "                    print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "                # truncate the strings for printing\n",
    "                truncated_row = truncate_string(row, max_str_length)\n",
    "                truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "                print(\n",
    "                    f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\"\n",
    "                )\n",
    "\n",
    "                # compute the word comparisons between the previous (with the coherence map)\n",
    "                # and the current (possibly the first sentence in a new segment)\n",
    "                word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "                    [*coherence_map, keywords_prev], keywords_current\n",
    "                )\n",
    "\n",
    "                similarities_with_coherence = [\n",
    "                    comparison[2] for comparison in word_comparisons_with_coherence\n",
    "                ]\n",
    "                \n",
    "                weighted_avg_similarity_with_coherence = get_weighted_average(\n",
    "                    similarities_with_coherence, weights\n",
    "                )\n",
    "                print(f\"weighted: {weighted_avg_similarity_with_coherence}\")\n",
    "\n",
    "                # if the two sentences are similar, create a cohesive prediction\n",
    "                # otherwise, predict a new segment\n",
    "                if weighted_avg_similarity_with_coherence > threshold:\n",
    "                    print(\n",
    "                        f\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                    )\n",
    "                    predictions.append((weighted_avg_similarity_with_coherence, 0))\n",
    "                else:\n",
    "                    if coherence_dump_on_prediction:\n",
    "                        # start of a new segment, empty the map\n",
    "                        coherence_map = []\n",
    "                    print(\n",
    "                        f\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                    )\n",
    "                    predictions.append((weighted_avg_similarity_with_coherence, 1))\n",
    "\n",
    "                thresholds.append(weighted_avg_similarity_with_coherence)\n",
    "                print(\"===============================================\")\n",
    "\n",
    "                prev_sentence = row\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca71b4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0, Prediction: 0\n",
      "Sample Number: 1\n",
      "Coherence Map: [['chickasaw', 'ecu', 'ehacoffice', 'legalshield']], KW Curr: ['chickasaw', 'legalshield', 'plasticware', 'wrangler']\n",
      "weighted: tensor([0.4932])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4932])\n",
      "===============================================\n",
      "Sample Number: 2\n",
      "Coherence Map: [['chickasaw', 'ecu', 'ehacoffice', 'legalshield'], ['glenwood', 'ecu', 'ehacoffice', 'cartography']], KW Curr: ['ecu', 'ehacoffice', 'cartography', 'accreditation']\n",
      "weighted: tensor([0.5707])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5707])\n",
      "===============================================\n",
      "Sample Number: 3\n",
      "Coherence Map: [['chickasaw', 'ecu', 'ehacoffice', 'legalshield'], ['glenwood', 'ecu', 'ehacoffice', 'cartography'], ['pontotoc', 'glenwood', 'technology', 'area']], KW Curr: ['glenwood', 'schools', 'high', 'secondary']\n",
      "weighted: tensor([0.6830])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6830])\n",
      "===============================================\n",
      "Sample Number: 4\n",
      "Coherence Map: [['chickasaw', 'ecu', 'ehacoffice', 'legalshield'], ['glenwood', 'ecu', 'ehacoffice', 'cartography'], ['pontotoc', 'glenwood', 'technology', 'area'], ['pontotoc', 'crossword', 'technology', 'area']], KW Curr: ['pontotoc', 'technology', 'area', 'ada']\n",
      "weighted: tensor([0.6380])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6380])\n",
      "===============================================\n",
      "Sample Number: 5\n",
      "Coherence Map: [['chickasaw', 'ecu', 'ehacoffice', 'legalshield'], ['glenwood', 'ecu', 'ehacoffice', 'cartography'], ['pontotoc', 'glenwood', 'technology', 'area'], ['pontotoc', 'crossword', 'technology', 'area'], ['tehuelche', 'chubut', 'gobernación', 'crossword']], KW Curr: ['crossword', 'imprisonments', 'palindrome', 'denice']\n",
      "weighted: tensor([0.4998])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4998])\n",
      "===============================================\n",
      "Sample Number: 6\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['glenwood', 'ecu', 'ehacoffice', 'cartography'], ['pontotoc', 'glenwood', 'technology', 'area'], ['pontotoc', 'crossword', 'technology', 'area'], ['tehuelche', 'chubut', 'gobernación', 'crossword'], ['tehuelche', 'afrikaners', 'chubut', 'afrikaner']], KW Curr: ['tehuelche', 'chubut', 'gobernación', 'utensils']\n",
      "weighted: tensor([0.4420])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.4420])\n",
      "===============================================\n",
      "Sample Number: 7\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['pontotoc', 'glenwood', 'technology', 'area'], ['pontotoc', 'crossword', 'technology', 'area'], ['tehuelche', 'chubut', 'gobernación', 'crossword'], ['tehuelche', 'afrikaners', 'chubut', 'afrikaner'], ['afrikaners', 'evapotranspiration', 'afrikaner', 'impediment']], KW Curr: ['afrikaners', 'afrikaner', 'impediment', 'rivadavia']\n",
      "weighted: tensor([0.5155])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5155])\n",
      "===============================================\n",
      "Sample Number: 8\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['pontotoc', 'crossword', 'technology', 'area'], ['tehuelche', 'chubut', 'gobernación', 'crossword'], ['tehuelche', 'afrikaners', 'chubut', 'afrikaner'], ['afrikaners', 'evapotranspiration', 'afrikaner', 'impediment'], ['patagonia', 'evapotranspiration', 'chubut', 'rivadavia']], KW Curr: ['evapotranspiration', 'rivadavia', 'precipitation', 'climate']\n",
      "weighted: tensor([0.4916])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4916])\n",
      "===============================================\n",
      "Sample Number: 9\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['tehuelche', 'chubut', 'gobernación', 'crossword'], ['tehuelche', 'afrikaners', 'chubut', 'afrikaner'], ['afrikaners', 'evapotranspiration', 'afrikaner', 'impediment'], ['patagonia', 'evapotranspiration', 'chubut', 'rivadavia'], ['viviendas', 'cayetano', 'ceferino', 'pueyrredon']], KW Curr: ['patagonia', 'chubut', 'rivadavia', 'fuego']\n",
      "weighted: tensor([0.5134])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5134])\n",
      "===============================================\n",
      "Sample Number: 0\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['tehuelche', 'afrikaners', 'chubut', 'afrikaner'], ['afrikaners', 'evapotranspiration', 'afrikaner', 'impediment'], ['patagonia', 'evapotranspiration', 'chubut', 'rivadavia'], ['viviendas', 'cayetano', 'ceferino', 'pueyrredon'], ['metallurgical', 'viviendas', 'cayetano', 'ceferino']], KW Curr: ['viviendas', 'cayetano', 'ceferino', 'pueyrredon']\n",
      "weighted: tensor([0.5510])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5510])\n",
      "===============================================\n",
      "Sample Number: 1\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['afrikaners', 'evapotranspiration', 'afrikaner', 'impediment'], ['patagonia', 'evapotranspiration', 'chubut', 'rivadavia'], ['viviendas', 'cayetano', 'ceferino', 'pueyrredon'], ['metallurgical', 'viviendas', 'cayetano', 'ceferino'], ['metallurgical', 'yacimientos', 'petrolíferos', 'refrigerated']], KW Curr: ['metallurgical', 'refrigerated', 'industry', 'salter']\n",
      "weighted: tensor([0.5289])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5289])\n",
      "===============================================\n",
      "Sample Number: 2\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['patagonia', 'evapotranspiration', 'chubut', 'rivadavia'], ['viviendas', 'cayetano', 'ceferino', 'pueyrredon'], ['metallurgical', 'viviendas', 'cayetano', 'ceferino'], ['metallurgical', 'yacimientos', 'petrolíferos', 'refrigerated'], ['yacimientos', 'petrolíferos', 'chubut', 'yrigoyen']], KW Curr: ['yacimientos', 'petrolíferos', 'yrigoyen', 'chubut']\n",
      "weighted: tensor([0.5649])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5649])\n",
      "===============================================\n",
      "Sample Number: 3\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['viviendas', 'cayetano', 'ceferino', 'pueyrredon'], ['metallurgical', 'viviendas', 'cayetano', 'ceferino'], ['metallurgical', 'yacimientos', 'petrolíferos', 'refrigerated'], ['yacimientos', 'petrolíferos', 'chubut', 'yrigoyen'], ['chubut', 'h310', 'h314', 'rivadavia']], KW Curr: ['chubut', 'rivadavia', 'comodoro', 'argentina']\n",
      "weighted: tensor([0.4907])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4907])\n",
      "===============================================\n",
      "Sample Number: 4\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['metallurgical', 'viviendas', 'cayetano', 'ceferino'], ['metallurgical', 'yacimientos', 'petrolíferos', 'refrigerated'], ['yacimientos', 'petrolíferos', 'chubut', 'yrigoyen'], ['chubut', 'h310', 'h314', 'rivadavia'], ['h310', 'h314', 'rivadavia', 'cartography']], KW Curr: ['h310', 'h314', 'rivadavia', 'cartography']\n",
      "weighted: tensor([0.5299])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5299])\n",
      "===============================================\n",
      "Sample Number: 5\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['metallurgical', 'yacimientos', 'petrolíferos', 'refrigerated'], ['yacimientos', 'petrolíferos', 'chubut', 'yrigoyen'], ['chubut', 'h310', 'h314', 'rivadavia'], ['h310', 'h314', 'rivadavia', 'cartography'], ['rivadavia', 'borja', 'buoys', 'maciel']], KW Curr: ['rivadavia', 'maciel', 'comodoro', 'built']\n",
      "weighted: tensor([0.5609])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5609])\n",
      "===============================================\n",
      "Sample Number: 6\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['yacimientos', 'petrolíferos', 'chubut', 'yrigoyen'], ['chubut', 'h310', 'h314', 'rivadavia'], ['h310', 'h314', 'rivadavia', 'cartography'], ['rivadavia', 'borja', 'buoys', 'maciel'], ['puzolanic', 'petroquimica', 'bricklaying', 'borja']], KW Curr: ['borja', 'buoys', 'rivadavia', 'caleta']\n",
      "weighted: tensor([0.5188])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5188])\n",
      "===============================================\n",
      "Sample Number: 7\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['chubut', 'h310', 'h314', 'rivadavia'], ['h310', 'h314', 'rivadavia', 'cartography'], ['rivadavia', 'borja', 'buoys', 'maciel'], ['puzolanic', 'petroquimica', 'bricklaying', 'borja'], ['puzolanic', 'petroquimica', 'bricklaying', 'rivadavia']], KW Curr: ['puzolanic', 'petroquimica', 'bricklaying', 'rivadavia']\n",
      "weighted: tensor([0.4992])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4992])\n",
      "===============================================\n",
      "Sample Number: 8\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['h310', 'h314', 'rivadavia', 'cartography'], ['rivadavia', 'borja', 'buoys', 'maciel'], ['puzolanic', 'petroquimica', 'bricklaying', 'borja'], ['puzolanic', 'petroquimica', 'bricklaying', 'rivadavia'], ['universitario', 'ferroviario', 'esgrima', 'nautico']], KW Curr: ['rivadavia', 'comodoro', 'generators', 'kw']\n",
      "weighted: tensor([0.4903])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4903])\n",
      "===============================================\n",
      "Sample Number: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['rivadavia', 'borja', 'buoys', 'maciel'], ['puzolanic', 'petroquimica', 'bricklaying', 'borja'], ['puzolanic', 'petroquimica', 'bricklaying', 'rivadavia'], ['universitario', 'ferroviario', 'esgrima', 'nautico'], ['strunkovka', 'zolotonosha', 'universitario', 'ferroviario']], KW Curr: ['universitario', 'ferroviario', 'esgrima', 'nautico']\n",
      "weighted: tensor([0.4636])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.4636])\n",
      "===============================================\n",
      "Sample Number: 0\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['puzolanic', 'petroquimica', 'bricklaying', 'borja'], ['puzolanic', 'petroquimica', 'bricklaying', 'rivadavia'], ['universitario', 'ferroviario', 'esgrima', 'nautico'], ['strunkovka', 'zolotonosha', 'universitario', 'ferroviario'], ['strunkovka', 'zolotonosha', '1576', 'chattanooga']], KW Curr: ['strunkovka', 'zolotonosha', '1576', 'magdeburg']\n",
      "weighted: tensor([0.5237])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.5237])\n",
      "===============================================\n",
      "Sample Number: 1\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['puzolanic', 'petroquimica', 'bricklaying', 'rivadavia'], ['universitario', 'ferroviario', 'esgrima', 'nautico'], ['strunkovka', 'zolotonosha', 'universitario', 'ferroviario'], ['strunkovka', 'zolotonosha', '1576', 'chattanooga'], ['5479', 'hohenwald', '5520', 'census']], KW Curr: ['chattanooga', '1878', 'town', 'immigrants']\n",
      "weighted: tensor([0.5609])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5609])\n",
      "===============================================\n",
      "Sample Number: 2\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['universitario', 'ferroviario', 'esgrima', 'nautico'], ['strunkovka', 'zolotonosha', 'universitario', 'ferroviario'], ['strunkovka', 'zolotonosha', '1576', 'chattanooga'], ['5479', 'hohenwald', '5520', 'census'], ['5479', 'hohenwald', '5520', '861']], KW Curr: ['5479', 'hohenwald', '5520', 'census']\n",
      "weighted: tensor([0.6522])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6522])\n",
      "===============================================\n",
      "Sample Number: 3\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['strunkovka', 'zolotonosha', 'universitario', 'ferroviario'], ['strunkovka', 'zolotonosha', '1576', 'chattanooga'], ['5479', 'hohenwald', '5520', 'census'], ['5479', 'hohenwald', '5520', '861'], ['976761', '279574', 'mi²', '861']], KW Curr: ['861', '391', '534', '989']\n",
      "weighted: tensor([0.6790])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.6790])\n",
      "===============================================\n",
      "Sample Number: 4\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['strunkovka', 'zolotonosha', '1576', 'chattanooga'], ['5479', 'hohenwald', '5520', 'census'], ['5479', 'hohenwald', '5520', '861'], ['976761', '279574', 'mi²', '861'], ['976761', '279574', 'mi²', '793']], KW Curr: ['976761', '279574', 'mi²', 'km²']\n",
      "weighted: tensor([0.6801])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6801])\n",
      "===============================================\n",
      "Sample Number: 5\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['5479', 'hohenwald', '5520', 'census'], ['5479', 'hohenwald', '5520', '861'], ['976761', '279574', 'mi²', '861'], ['976761', '279574', 'mi²', '793'], ['sunbeam', '793', '097', '524']], KW Curr: ['793', '097', '524', 'mi²']\n",
      "weighted: tensor([0.6348])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6348])\n",
      "===============================================\n",
      "Sample Number: 6\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['5479', 'hohenwald', '5520', '861'], ['976761', '279574', 'mi²', '861'], ['976761', '279574', 'mi²', '793'], ['sunbeam', '793', '097', '524'], ['sylva', 'school', 'west', 'springs']], KW Curr: ['sunbeam', 'dunlap', 'downsized', 'employment']\n",
      "weighted: tensor([0.6480])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6480])\n",
      "===============================================\n",
      "Sample Number: 7\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['976761', '279574', 'mi²', '861'], ['976761', '279574', 'mi²', '793'], ['sunbeam', '793', '097', '524'], ['sylva', 'school', 'west', 'springs'], ['sylva', 'school', 'west', 'springs']], KW Curr: ['sylva', 'school', 'west', 'springs']\n",
      "weighted: tensor([0.7189])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7189])\n",
      "===============================================\n",
      "Sample Number: 8\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['976761', '279574', 'mi²', '793'], ['sunbeam', '793', '097', '524'], ['sylva', 'school', 'west', 'springs'], ['sylva', 'school', 'west', 'springs'], ['wabash', 'niewerth', 'welcomes', 'towpath']], KW Curr: ['climate', 'winters', 'humid', 'hot']\n",
      "weighted: tensor([0.5866])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.5866])\n",
      "===============================================\n",
      "Sample Number: 9\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['sunbeam', '793', '097', '524'], ['sylva', 'school', 'west', 'springs'], ['sylva', 'school', 'west', 'springs'], ['wabash', 'niewerth', 'welcomes', 'towpath'], ['wabash', 'niewerth', 'census', 'land']], KW Curr: ['wabash', 'niewerth', 'welcomes', 'towpath']\n",
      "weighted: tensor([0.5103])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5103])\n",
      "===============================================\n",
      "Sample Number: 0\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['sylva', 'school', 'west', 'springs'], ['sylva', 'school', 'west', 'springs'], ['wabash', 'niewerth', 'welcomes', 'towpath'], ['wabash', 'niewerth', 'census', 'land'], ['census', 'land', 'area', 'delphi']], KW Curr: ['census', 'land', 'area', 'delphi']\n",
      "weighted: tensor([0.6739])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6739])\n",
      "===============================================\n",
      "Sample Number: 1\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['sylva', 'school', 'west', 'springs'], ['wabash', 'niewerth', 'welcomes', 'towpath'], ['wabash', 'niewerth', 'census', 'land'], ['census', 'land', 'area', 'delphi'], ['485', '454', '893', '748']], KW Curr: ['893', '694', 'male', 'census']\n",
      "weighted: tensor([0.6896])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6896])\n",
      "===============================================\n",
      "Sample Number: 2\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['wabash', 'niewerth', 'welcomes', 'towpath'], ['wabash', 'niewerth', 'census', 'land'], ['census', 'land', 'area', 'delphi'], ['485', '454', '893', '748'], ['schools', 'delphi', 'school', 'high']], KW Curr: ['485', '454', '748', '015']\n",
      "weighted: tensor([0.6026])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6026])\n",
      "===============================================\n",
      "Sample Number: 3\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['wabash', 'niewerth', 'census', 'land'], ['census', 'land', 'area', 'delphi'], ['485', '454', '893', '748'], ['schools', 'delphi', 'school', 'high'], ['schools', 'delphi', 'zagrosh', 'school']], KW Curr: ['schools', 'delphi', 'school', 'high']\n",
      "weighted: tensor([0.6697])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.6697])\n",
      "===============================================\n",
      "Sample Number: 4\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['census', 'land', 'area', 'delphi'], ['485', '454', '893', '748'], ['schools', 'delphi', 'school', 'high'], ['schools', 'delphi', 'zagrosh', 'school'], ['اچمی', 'larestani', 'ajami', 'language']], KW Curr: ['zagrosh', 'summer', 'winter', 'snowfall']\n",
      "weighted: tensor([0.5723])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5723])\n",
      "===============================================\n",
      "Sample Number: 5\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['485', '454', '893', '748'], ['schools', 'delphi', 'school', 'high'], ['schools', 'delphi', 'zagrosh', 'school'], ['اچمی', 'larestani', 'ajami', 'language'], ['اچمی', 'larestani', 'ajami', 'mcarthur']], KW Curr: ['اچمی', 'larestani', 'ajami', 'language']\n",
      "weighted: tensor([0.5271])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.5271])\n",
      "===============================================\n",
      "Sample Number: 6\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['schools', 'delphi', 'school', 'high'], ['schools', 'delphi', 'zagrosh', 'school'], ['اچمی', 'larestani', 'ajami', 'language'], ['اچمی', 'larestani', 'ajami', 'mcarthur'], ['mcarthur', 'census', 'land', 'city']], KW Curr: ['mcarthur', 'story', 'names', '1880']\n",
      "weighted: tensor([0.5424])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5424])\n",
      "===============================================\n",
      "Sample Number: 7\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['schools', 'delphi', 'zagrosh', 'school'], ['اچمی', 'larestani', 'ajami', 'language'], ['اچمی', 'larestani', 'ajami', 'mcarthur'], ['mcarthur', 'census', 'land', 'city'], ['census', 'land', 'city', 'male']], KW Curr: ['census', 'land', 'city', 'states']\n",
      "weighted: tensor([0.7294])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7294])\n",
      "===============================================\n",
      "Sample Number: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['اچمی', 'larestani', 'ajami', 'language'], ['اچمی', 'larestani', 'ajami', 'mcarthur'], ['mcarthur', 'census', 'land', 'city'], ['census', 'land', 'city', 'male'], ['594', '642', 'male', 'census']], KW Curr: ['male', 'census', 'female', 'population']\n",
      "weighted: tensor([0.7001])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.7001])\n",
      "===============================================\n",
      "Sample Number: 9\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['اچمی', 'larestani', 'ajami', 'mcarthur'], ['mcarthur', 'census', 'land', 'city'], ['census', 'land', 'city', 'male'], ['594', '642', 'male', 'census'], ['dryland', 'stateline', 'vansycle', 'canola']], KW Curr: ['594', '642', 'census', 'female']\n",
      "weighted: tensor([0.6192])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6192])\n",
      "===============================================\n",
      "Sample Number: 0\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['mcarthur', 'census', 'land', 'city'], ['census', 'land', 'city', 'male'], ['594', '642', 'male', 'census'], ['dryland', 'stateline', 'vansycle', 'canola'], ['dryland', 'wheatstock', 'stateline', 'vansycle']], KW Curr: ['dryland', 'stateline', 'vansycle', 'canola']\n",
      "weighted: tensor([0.6538])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6538])\n",
      "===============================================\n",
      "Sample Number: 1\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['census', 'land', 'city', 'male'], ['594', '642', 'male', 'census'], ['dryland', 'stateline', 'vansycle', 'canola'], ['dryland', 'wheatstock', 'stateline', 'vansycle'], ['são', '605', 'wheatstock', 'weekend']], KW Curr: ['wheatstock', 'weekend', 'hosts', 'august']\n",
      "weighted: tensor([0.6541])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.6541])\n",
      "===============================================\n",
      "Sample Number: 2\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['594', '642', 'male', 'census'], ['dryland', 'stateline', 'vansycle', 'canola'], ['dryland', 'wheatstock', 'stateline', 'vansycle'], ['são', '605', 'wheatstock', 'weekend'], ['soybeans', 'agricultural', 'são', 'economy']], KW Curr: ['são', '605', 'altitude', 'state']\n",
      "weighted: tensor([0.6116])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.6116])\n",
      "===============================================\n",
      "Sample Number: 3\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['dryland', 'stateline', 'vansycle', 'canola'], ['dryland', 'wheatstock', 'stateline', 'vansycle'], ['são', '605', 'wheatstock', 'weekend'], ['soybeans', 'agricultural', 'são', 'economy'], ['soybeans', 'ituverava', 'agricultural', 'guarani']], KW Curr: ['soybeans', 'agricultural', 'economy', 'cultivation']\n",
      "weighted: tensor([0.5665])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5665])\n",
      "===============================================\n",
      "Sample Number: 4\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['dryland', 'wheatstock', 'stateline', 'vansycle'], ['são', '605', 'wheatstock', 'weekend'], ['soybeans', 'agricultural', 'são', 'economy'], ['soybeans', 'ituverava', 'agricultural', 'guarani'], ['ituverava', 'σκόδρα', 'guarani', 'skodra']], KW Curr: ['ituverava', 'guarani', 'tupi', 'carmo']\n",
      "weighted: tensor([0.4977])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4977])\n",
      "===============================================\n",
      "Sample Number: 5\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['são', '605', 'wheatstock', 'weekend'], ['soybeans', 'agricultural', 'são', 'economy'], ['soybeans', 'ituverava', 'agricultural', 'guarani'], ['ituverava', 'σκόδρα', 'guarani', 'skodra'], ['σκόδρα', 'shkodër', 'skodra', 'phonological']], KW Curr: ['σκόδρα', 'skodra', 'phonological', 'skodrians']\n",
      "weighted: tensor([0.4806])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4806])\n",
      "===============================================\n",
      "Sample Number: 6\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['soybeans', 'agricultural', 'são', 'economy'], ['soybeans', 'ituverava', 'agricultural', 'guarani'], ['ituverava', 'σκόδρα', 'guarani', 'skodra'], ['σκόδρα', 'shkodër', 'skodra', 'phonological'], ['wettest', 'shkodër', 'mbishkodra', 'hydrologically']], KW Curr: ['shkodër', 'mbishkodra', 'hydrologically', 'maranaj']\n",
      "weighted: tensor([0.5238])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5238])\n",
      "===============================================\n",
      "Sample Number: 7\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['soybeans', 'ituverava', 'agricultural', 'guarani'], ['ituverava', 'σκόδρα', 'guarani', 'skodra'], ['σκόδρα', 'shkodër', 'skodra', 'phonological'], ['wettest', 'shkodër', 'mbishkodra', 'hydrologically'], ['illyrian', 'wettest', 'rozafa', 'teuta']], KW Curr: ['wettest', 'precipitation', 'shkodër', 'climate']\n",
      "weighted: tensor([0.4999])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4999])\n",
      "===============================================\n",
      "Sample Number: 8\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['ituverava', 'σκόδρα', 'guarani', 'skodra'], ['σκόδρα', 'shkodër', 'skodra', 'phonological'], ['wettest', 'shkodër', 'mbishkodra', 'hydrologically'], ['illyrian', 'wettest', 'rozafa', 'teuta'], ['manoeuvres', 'illyrian', 'rozafa', 'sieges']], KW Curr: ['illyrian', 'rozafa', 'teuta', 'ardiaei']\n",
      "weighted: tensor([0.5664])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5664])\n",
      "===============================================\n",
      "Sample Number: 9\n",
      "pruning... 6\n",
      "done pruning... 5\n",
      "Coherence Map: [['σκόδρα', 'shkodër', 'skodra', 'phonological'], ['wettest', 'shkodër', 'mbishkodra', 'hydrologically'], ['illyrian', 'wettest', 'rozafa', 'teuta'], ['manoeuvres', 'illyrian', 'rozafa', 'sieges'], ['manoeuvres', 'bashkimi', 'shkodër', 'prizren']], KW Curr: ['manoeuvres', 'sieges', '1485', 'mehmed']\n",
      "weighted: tensor([0.5007])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5007])\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 58;\n",
       "                var nbb_unformatted_code = \"start = 100\\nnum_samples = 50\\nmax_tokens = 128  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"start = 100\\nnum_samples = 50\\nmax_tokens = 128  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 100\n",
    "num_samples = 50\n",
    "max_tokens = 128  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "true_labels = text_labels[start : start + num_samples]\n",
    "\n",
    "predictions = coherence_tester(\n",
    "    text_data[start : start + num_samples],\n",
    "    true_labels,\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 59;\n",
       "                var nbb_unformatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_formatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print([x[1] for x in predictions])\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61e7863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_formatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_string = \"\".join(str([x[1] for x in predictions]))\n",
    "true_string = \"\".join(str(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19af16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db43c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "wd = 0.2876712328767123\n",
      "pk = 0.273972602739726\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb16194",
   "metadata": {},
   "source": [
    "## Prediction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81b6b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 46;\n",
       "                var nbb_unformatted_code = \"pred_thresholds = [\\n    0.2,\\n    0.21,\\n    0.22,\\n    0.23,\\n    0.24,\\n    0.25,\\n    0.26,\\n    0.27,\\n    0.28,\\n    0.29,\\n    0.3,\\n]  # bert base uncased\\npred_thresholds = [\\n    0.4,\\n    0.41,\\n    0.42,\\n    0.43,\\n    0.44,\\n    0.45,\\n    0.46,\\n    0.47,\\n    0.48,\\n    0.49,\\n    0.5,\\n]  # labse\\n# pred_thresholds = [\\n#     0.06,\\n#     0.07,\\n#     0.08,\\n#     0.09,\\n#     0.1,\\n#     0.11,\\n#     0.12,\\n#     0.13,\\n#     0.14,\\n#     0.15,\\n#     0.16,\\n#     0.17,\\n#     0.18,\\n#     0.19,\\n#     0.2,\\n#     0.11,\\n#     0.06,\\n# ]  # sentence-transformers\\n# pred_thresholds = [\\n#     0.6,\\n#     0.61,\\n#     0.62,\\n#     0.63,\\n#     0.64,\\n#     0.65,\\n#     0.66,\\n#     0.67,\\n#     0.68,\\n#     0.69,\\n#     0.7,\\n# ]  # USE\\n# pred_thresholds = [\\n#     0.65,\\n#     0.66,\\n#     0.67,\\n#     0.68,\\n#     0.69,\\n#     0.7,\\n#     0.71,\\n#     0.72,\\n#     0.73,\\n#     0.74,\\n#     0.75,\\n#     0.76,\\n#     0.77,\\n#     0.78,\\n#     0.79,\\n#     0.64,\\n# ]  # Roberta\";\n",
       "                var nbb_formatted_code = \"pred_thresholds = [\\n    0.2,\\n    0.21,\\n    0.22,\\n    0.23,\\n    0.24,\\n    0.25,\\n    0.26,\\n    0.27,\\n    0.28,\\n    0.29,\\n    0.3,\\n]  # bert base uncased\\npred_thresholds = [\\n    0.4,\\n    0.41,\\n    0.42,\\n    0.43,\\n    0.44,\\n    0.45,\\n    0.46,\\n    0.47,\\n    0.48,\\n    0.49,\\n    0.5,\\n]  # labse\\n# pred_thresholds = [\\n#     0.06,\\n#     0.07,\\n#     0.08,\\n#     0.09,\\n#     0.1,\\n#     0.11,\\n#     0.12,\\n#     0.13,\\n#     0.14,\\n#     0.15,\\n#     0.16,\\n#     0.17,\\n#     0.18,\\n#     0.19,\\n#     0.2,\\n#     0.11,\\n#     0.06,\\n# ]  # sentence-transformers\\n# pred_thresholds = [\\n#     0.6,\\n#     0.61,\\n#     0.62,\\n#     0.63,\\n#     0.64,\\n#     0.65,\\n#     0.66,\\n#     0.67,\\n#     0.68,\\n#     0.69,\\n#     0.7,\\n# ]  # USE\\n# pred_thresholds = [\\n#     0.65,\\n#     0.66,\\n#     0.67,\\n#     0.68,\\n#     0.69,\\n#     0.7,\\n#     0.71,\\n#     0.72,\\n#     0.73,\\n#     0.74,\\n#     0.75,\\n#     0.76,\\n#     0.77,\\n#     0.78,\\n#     0.79,\\n#     0.64,\\n# ]  # Roberta\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_thresholds = [\n",
    "    0.2,\n",
    "    0.21,\n",
    "    0.22,\n",
    "    0.23,\n",
    "    0.24,\n",
    "    0.25,\n",
    "    0.26,\n",
    "    0.27,\n",
    "    0.28,\n",
    "    0.29,\n",
    "    0.3,\n",
    "]  # bert base uncased\n",
    "pred_thresholds = [\n",
    "    0.4,\n",
    "    0.41,\n",
    "    0.42,\n",
    "    0.43,\n",
    "    0.44,\n",
    "    0.45,\n",
    "    0.46,\n",
    "    0.47,\n",
    "    0.48,\n",
    "    0.49,\n",
    "    0.5,\n",
    "]  # labse\n",
    "# pred_thresholds = [\n",
    "#     0.06,\n",
    "#     0.07,\n",
    "#     0.08,\n",
    "#     0.09,\n",
    "#     0.1,\n",
    "#     0.11,\n",
    "#     0.12,\n",
    "#     0.13,\n",
    "#     0.14,\n",
    "#     0.15,\n",
    "#     0.16,\n",
    "#     0.17,\n",
    "#     0.18,\n",
    "#     0.19,\n",
    "#     0.2,\n",
    "#     0.11,\n",
    "#     0.06,\n",
    "# ]  # sentence-transformers\n",
    "# pred_thresholds = [\n",
    "#     0.6,\n",
    "#     0.61,\n",
    "#     0.62,\n",
    "#     0.63,\n",
    "#     0.64,\n",
    "#     0.65,\n",
    "#     0.66,\n",
    "#     0.67,\n",
    "#     0.68,\n",
    "#     0.69,\n",
    "#     0.7,\n",
    "# ]  # USE\n",
    "# pred_thresholds = [\n",
    "#     0.65,\n",
    "#     0.66,\n",
    "#     0.67,\n",
    "#     0.68,\n",
    "#     0.69,\n",
    "#     0.7,\n",
    "#     0.71,\n",
    "#     0.72,\n",
    "#     0.73,\n",
    "#     0.74,\n",
    "#     0.75,\n",
    "#     0.76,\n",
    "#     0.77,\n",
    "#     0.78,\n",
    "#     0.79,\n",
    "#     0.64,\n",
    "# ]  # Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9cebcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_thresh = 0.4\n",
      "k = 5\n",
      "wd = 0.3082191780821918\n",
      "pk = 0.3082191780821918\n",
      "===========================================\n",
      "pred_thresh = 0.41\n",
      "k = 5\n",
      "wd = 0.3082191780821918\n",
      "pk = 0.3082191780821918\n",
      "===========================================\n",
      "pred_thresh = 0.42\n",
      "k = 5\n",
      "wd = 0.3082191780821918\n",
      "pk = 0.3082191780821918\n",
      "===========================================\n",
      "pred_thresh = 0.43\n",
      "k = 5\n",
      "wd = 0.3082191780821918\n",
      "pk = 0.3082191780821918\n",
      "===========================================\n",
      "pred_thresh = 0.44\n",
      "k = 5\n",
      "wd = 0.3082191780821918\n",
      "pk = 0.3082191780821918\n",
      "===========================================\n",
      "pred_thresh = 0.45\n",
      "k = 5\n",
      "wd = 0.3150684931506849\n",
      "pk = 0.3150684931506849\n",
      "===========================================\n",
      "pred_thresh = 0.46\n",
      "k = 5\n",
      "wd = 0.3150684931506849\n",
      "pk = 0.3150684931506849\n",
      "===========================================\n",
      "pred_thresh = 0.47\n",
      "k = 5\n",
      "wd = 0.2945205479452055\n",
      "pk = 0.2808219178082192\n",
      "===========================================\n",
      "pred_thresh = 0.48\n",
      "k = 5\n",
      "wd = 0.2945205479452055\n",
      "pk = 0.2808219178082192\n",
      "===========================================\n",
      "pred_thresh = 0.49\n",
      "k = 5\n",
      "wd = 0.3698630136986301\n",
      "pk = 0.3424657534246575\n",
      "===========================================\n",
      "pred_thresh = 0.5\n",
      "k = 5\n",
      "wd = 0.4315068493150685\n",
      "pk = 0.3904109589041096\n",
      "===========================================\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 47;\n",
       "                var nbb_unformatted_code = \"for pred_thresh in pred_thresholds:\\n    modified_predictions = [\\n        1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n    ]\\n\\n    pred_string = \\\"\\\".join(str(modified_predictions))\\n    true_string = \\\"\\\".join(str(true_labels))\\n\\n    avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\\n\\n    wd_score = windowdiff(pred_string, true_string, avg_k)\\n    pk_score = pk(pred_string, true_string, avg_k)\\n\\n    print(f\\\"pred_thresh = {pred_thresh}\\\")\\n    print(f\\\"k = {avg_k}\\\")\\n    print(f\\\"wd = {wd_score}\\\")\\n    print(f\\\"pk = {pk_score}\\\")\\n    print(\\\"===========================================\\\")\";\n",
       "                var nbb_formatted_code = \"for pred_thresh in pred_thresholds:\\n    modified_predictions = [\\n        1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n    ]\\n\\n    pred_string = \\\"\\\".join(str(modified_predictions))\\n    true_string = \\\"\\\".join(str(true_labels))\\n\\n    avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\\n\\n    wd_score = windowdiff(pred_string, true_string, avg_k)\\n    pk_score = pk(pred_string, true_string, avg_k)\\n\\n    print(f\\\"pred_thresh = {pred_thresh}\\\")\\n    print(f\\\"k = {avg_k}\\\")\\n    print(f\\\"wd = {wd_score}\\\")\\n    print(f\\\"pk = {pk_score}\\\")\\n    print(\\\"===========================================\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for pred_thresh in pred_thresholds:\n",
    "    modified_predictions = [\n",
    "        1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\n",
    "    ]\n",
    "\n",
    "    pred_string = \"\".join(str(modified_predictions))\n",
    "    true_string = \"\".join(str(true_labels))\n",
    "\n",
    "    avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\n",
    "\n",
    "    wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "    pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "    print(f\"pred_thresh = {pred_thresh}\")\n",
    "    print(f\"k = {avg_k}\")\n",
    "    print(f\"wd = {wd_score}\")\n",
    "    print(f\"pk = {pk_score}\")\n",
    "    print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a2f65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 107;\n",
       "                var nbb_unformatted_code = \"from sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\nprint(pred_string)\\nprint(true_string)\\n\\ntn, fp, fn, tp = confusion_matrix(true_labels, modified_predictions).ravel()\\nprecision, recall, f1, _ = precision_recall_fscore_support(\\n    true_labels, modified_predictions, average=\\\"macro\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"from sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\nprint(pred_string)\\nprint(true_string)\\n\\ntn, fp, fn, tp = confusion_matrix(true_labels, modified_predictions).ravel()\\nprecision, recall, f1, _ = precision_recall_fscore_support(\\n    true_labels, modified_predictions, average=\\\"macro\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(pred_string)\n",
    "print(true_string)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, modified_predictions).ravel()\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    true_labels, modified_predictions, average=\"macro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b9dbb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 6\n",
      "wd = 0.3076751946607342\n",
      "pk = 0.30567296996662957\n",
      "tn = 1203\n",
      "fp = 55\n",
      "fn = 215\n",
      "tp = 27\n",
      "precision = 0.5888231449310262\n",
      "recall = 0.533925028577435\n",
      "f1 = 0.5328849028400597\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 108;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\\nprint(f\\\"tn = {tn}\\\")\\nprint(f\\\"fp = {fp}\\\")\\nprint(f\\\"fn = {fn}\\\")\\nprint(f\\\"tp = {tp}\\\")\\nprint(f\\\"precision = {precision}\\\")\\nprint(f\\\"recall = {recall}\\\")\\nprint(f\\\"f1 = {f1}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\\nprint(f\\\"tn = {tn}\\\")\\nprint(f\\\"fp = {fp}\\\")\\nprint(f\\\"fn = {fn}\\\")\\nprint(f\\\"tp = {tp}\\\")\\nprint(f\\\"precision = {precision}\\\")\\nprint(f\\\"recall = {recall}\\\")\\nprint(f\\\"f1 = {f1}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")\n",
    "print(f\"tn = {tn}\")\n",
    "print(f\"fp = {fp}\")\n",
    "print(f\"fn = {fn}\")\n",
    "print(f\"tp = {tp}\")\n",
    "print(f\"precision = {precision}\")\n",
    "print(f\"recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34585627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462a8434",
   "metadata": {},
   "source": [
    "## KeyBERT Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15e7648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 172;\n",
       "                var nbb_unformatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_formatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr = 230\n",
    "prev = curr - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c6434ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the keywords in 0.6567 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "['cantonese', 'languages', 'vietnamese', 'communes']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 205;\n",
       "                var nbb_unformatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_formatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohesion = coherence.get_coherence(\n",
    "    [text_data[curr], text_data[prev]], coherence_threshold=0.25\n",
    ")\n",
    "print([k[0] for k in cohesion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "357c0021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 206;\n",
       "                var nbb_unformatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_formatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the keywords for the current sentences\n",
    "keywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\n",
    "keywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\n",
    "\n",
    "# compute the word comparisons between the previous (with the coherence map)\n",
    "# and the current (possibly the first sentence in a new segment)\n",
    "word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "    [keywords_prev], keywords_current\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dd52c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('township', 0.2304),\n",
       "  ('communes', 0.1857),\n",
       "  ('hải', 0.1399),\n",
       "  ('wards', 0.1397),\n",
       "  ('đông', 0.1224)],\n",
       " [('cantonese', 0.5038),\n",
       "  ('mandarin', 0.464),\n",
       "  ('languages', 0.3483),\n",
       "  ('language', 0.343),\n",
       "  ('vietnamese', 0.3184)])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 207;\n",
       "                var nbb_unformatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_formatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# KeyBERT Embedding Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "559ab602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 679;\n",
       "                var nbb_unformatted_code = \"docs = [\\n        \\\"Hi my name is Devarsh\\\",\\n        \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n        \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"docs = [\\n    \\\"Hi my name is Devarsh\\\",\\n    \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n    \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"Hi my name is Devarsh\",\n",
    "    \"Devarsh likes to play Basketball.\",\n",
    "    \"I love to watch Cricket.\",\n",
    "    \"I am a strong programmer. And my name is Devarsh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "00458200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 680;\n",
       "                var nbb_unformatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_formatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(\n",
    "    docs, min_df=1, stop_words=\"english\"\n",
    ")\n",
    "keywords = kw_model.extract_keywords(\n",
    "    docs,\n",
    "    min_df=1,\n",
    "    stop_words=\"english\",\n",
    "    doc_embeddings=doc_embeddings,\n",
    "    word_embeddings=word_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7d30bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 681;\n",
       "                var nbb_unformatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "018ee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 682;\n",
       "                var nbb_unformatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "80cbdc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('devarsh', 0.6267), ('hi', 0.5216)],\n",
       " [('devarsh', 0.6549),\n",
       "  ('basketball', 0.5558),\n",
       "  ('play', 0.3787),\n",
       "  ('likes', 0.2284)],\n",
       " [('cricket', 0.7118), ('watch', 0.3656), ('love', 0.307)],\n",
       " [('programmer', 0.5942), ('devarsh', 0.5528), ('strong', 0.3452)]]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 683;\n",
       "                var nbb_unformatted_code = \"keywords\";\n",
       "                var nbb_formatted_code = \"keywords\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "fd1ac50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 701;\n",
       "                var nbb_unformatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_formatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_keywords_with_embeddings_test(\n",
    "    data,\n",
    ") -> list[tuple[str, float, torch.Tensor]]:\n",
    "    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\n",
    "\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\n",
    "    )\n",
    "\n",
    "    keywords_with_embeddings = []\n",
    "    count = 0\n",
    "    print(len(word_embeddings))\n",
    "    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\n",
    "        for j, words in enumerate(kw):\n",
    "            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\n",
    "            count += 1\n",
    "\n",
    "    return keywords_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "d1bbf3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 702;\n",
       "                var nbb_unformatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_formatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = get_keywords_with_embeddings_test(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f1ea7b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 703;\n",
       "                var nbb_unformatted_code = \"len(embeddings)\";\n",
       "                var nbb_formatted_code = \"len(embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
